NOT FOR COMMERTIAL USE!!!
Written only to demonstrate the possibilities of recognition noisy digits

# Что это? 

Распознавание сильно зашумленных гауссовым шумом цифр с помощью инструментов контурного анализа.
Обучение проводилось частным случаем обучения по прецендентам[[8]](#literature-8).

Предварительная обработка осуществлялась оконными фильтрами. В общем случае был достигнут результат в 90% точности распознавания.

# Оглавление

- [Цели](#goals)
- [Общие положения](#general)
- [Алгоритм](#algo)
  - [Предварительная обработка изображения и выбор контура](#algo-preprocessing)
  - [Контурный анализ](#algo-ca)
  - [Обучение по методу ближайших соседей и адаптивные веса](#algo-precendent)
  - [Корректировки и хаки](#algo-hacks)
- [Результаты экспериментов](#test)
  - [Данные и гипотезы](#test-suppose)
  - [Обучение на «жестких весах»](#test-training-hard)
  - [Проверка на «жестких весах»](#test-verification-hard)  
  - [Обучение на «мягких весах»](#test-training-soft)
  - [Проверка на «мягких весах»](#test-verification-soft)
  - [Обучение на «жестких» и проверка на «мягких» весах](#test-soft-hard)
  - [Общий результат на выборке в 3000](#test-full)
- [Инструкция по использованию](https://github.com/levabd/digit-symbol-recogniser/wiki)
  - [Основной интерфейс (обучение и распознавание)](https://github.com/levabd/digit-symbol-recogniser/wiki/%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D0%BD%D0%BE%D0%B9-%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D1%84%D0%B5%D0%B9%D1%81-(%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5-%D0%B8-%D1%80%D0%B0%D1%81%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5))
  - [Просмотр и редактирования прецендентов](https://github.com/levabd/digit-symbol-recogniser/wiki/%D0%9F%D1%80%D0%BE%D1%81%D0%BC%D0%BE%D1%82%D1%80-%D0%B8-%D1%80%D0%B5%D0%B4%D0%B0%D0%BA%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F-%D0%BF%D1%80%D0%B5%D1%86%D0%B5%D0%BD%D0%B4%D0%B5%D0%BD%D1%82%D0%BE%D0%B2)
  - [Обучение](https://github.com/levabd/digit-symbol-recogniser/wiki/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5)
- [Выводы](#conclusion)
- [Литература](#literature)
- [License](#license)

# <a name="goals"></a> Цели

Сейчас все помешаны на нейронных сетях. Хотелось доказать что нейронные сети не являются серебряной пулей.
Также хотел понять насколько эффективен контурный анализ в качестве системы распознавания. Об этом в [Выводах](#conclusion)

# <a name="general"></a> Общие положения

Для интереса были выбраны сильно зашумленные адиттивным гауссовым шумом цифры [[1]](#literature-1). Также, для усложнения процесса распознавания было принято решение проверять сразу по 5 цифр, а не по одной. Все примеры в дальнейшем алгоритме будут проведены над следующим изображением:

![](./resources/doc/source.png?raw=true)

# <a name="algo"></a> Алгоритм

Общий подход состоит в следующем:
- Разделение и нахождение цифр;
- Предобработка цифр;
- Выделение сигнатур;
- Обучение сигнатур;

## <a name="algo-preprocessing"></a> Предварительная обработка изображения и выбор контура

Само мобой первый этап был преобразование изображения в бинарное, но даже не буду останавливаться на этом.

Далее были вырезаны цифры из изображений по их положению на изображении. Благо мы точно знаем их положение и они не меняются. В реальной жизни такое редкость, но задача выделение отдельных обьектов достаточно тривиальна. Взять тот же [Breadth-first search](https://en.wikipedia.org/wiki/Breadth-first_search). И снова таки, данная задача не является предметом эксперимента.

Так как в дальнейшем планировалось выделить в качестве сигнатур цифр использовать контур -- была добавлена исскуственная граница к каждому вырезанному изображению чтоб не получить побочные эффекты на этапе выделения контура за счет соединения искомого контура с границей на изображении.

Кроме того, изображение сильно зашумлено гауссовым шумом. Для того чтоб получить более вменяемое изображение был использован специально фильтр Kuwahara c ядром 2 [[2]](#literature-2). Фильтр был использован последовательно дважды, так как изображение было очень зашумлено.

В результате обработки фильтром Kuwahara мы получили следующие изображения:

![](./resources/doc/digit0.png?raw=true) ![](./resources/doc/digit1.png?raw=true) ![](./resources/doc/digit2.png?raw=true) ![](./resources/doc/digit3.png?raw=true) ![](./resources/doc/digit4.png?raw=true)

Как видим, на изображении остались мелкие «шумные» обьекты. Они будут препятствовать правильному выбору контура и его распознаванию. Удалим их.

![](./resources/doc/gauss0.png?raw=true) ![](./resources/doc/gauss1.png?raw=true) ![](./resources/doc/gauss2.png?raw=true) ![](./resources/doc/gauss3.png?raw=true) ![](./resources/doc/gauss4.png?raw=true)

Есть еще некоторые проблемы в результирующем решении. Контуры из-за шума иногда разрываются (посмотрите на цифру «7»). Это повлечет за собой неправильное распознавание (не вся цифра) и повышенное количество шаблонов в базе для распознавание. Сгладим немного контуры гауссовым размытием [[3]](#literature-3). Подобранные значения *сигма = 3* и *ядро = 4*. Выделим все присутствующие контуры на изображении.

![](./resources/doc/contour0.png?raw=true) ![](./resources/doc/contour1.png?raw=true) ![](./resources/doc/contour2.png?raw=true) ![](./resources/doc/contour3.png?raw=true) ![](./resources/doc/contour4.png?raw=true)

Теперь надо бы выбрать внешний контур силуэта цифры. Но на изображении некоторые цифры имеют разрывы, которые у нас нивелировались предварительной обработкой (9 и 3). Потому иногда требуется выбрать «внутренний» контур силуэта. Но внутренний контур иногда совсем не то что нам надо (круг внутри цифры 9). Потому используем следующий критерий для выбора контура:

> Если самый длинный контур более чем в 3 раза длиннее второго по длине контура -- выбираем для анализа самій длинный контур. Иначе -- второй по длине.

Размоем контур еще раз с помощью оконного фильтра Канни [[4]](#literature-4)

![](./resources/doc/lastContour0.png?raw=true) ![](./resources/doc/lastContour1.png?raw=true) ![](./resources/doc/lastContour2.png?raw=true) ![](./resources/doc/lastContour3.png?raw=true) ![](./resources/doc/lastContour4.png?raw=true)

С такими контурами уже более менее можно работать.

## <a name="algo-ca"></a> Контурный анализ

Итак вернемся к изначальному плану. Уже было проведено разделение и нахождение цифр. Произошла предварительная обработка цифр. Теперь требуется выделить сигнатуру для каждой цифры. Как известно классификатор на больших векторах признаков (все изображение) склонен к переобучению. Хорошей характеристикой для цифры я считаю ее контур. Потому будем использовать аппарат контурного анализа [[5]](#literature-5). К тому же контурный анализ позволяет эффективно решать основные проблемы распознавания образов – перенос, поворот и изменение масштаба изображения объекта.

Контур – это граница объекта, совокупность точек (пикселов), отделяющих объект от фона. Внутренние точки объекта во внимание не принимаются. Используется несколько способов кодирования контура – наиболее известны код Фримена, двумерное кодирование, полигональное кодирование. Но все эти способы кодирования не используются в контурном анализе. Вместо этого, в контурном анализе контур кодируется последовательностью, состоящей из комплексных чисел. На контуре фиксируется точка, которая называется начальной точкой. Затем, контур обходится (допустим – по часовой стрелке), и каждый вектор смещения записывается комплексным числом a+ib. Где a – смещение точки по оси X, а b – смещение по оси Y. Смещение берется относительно предыдущей точки.

![](./resources/doc/contour-doc-1.png?raw=true)

Каждый вектор контура будем называть элементарным вектором (ЭВ). А саму последовательность комплекснозначных чисел – вектор-контуром (ВК). Комплексное кодирование близко к двумерному кодированию, где контур определяется как совокупность ЭВ, представленных своими двумерными координатами. Но разница в том, что операция скалярного произведения для векторов и для комплексных чисел – различны. Именно это обстоятельство и дает преимущество методам контурного анализа.

У таких контуров есть ряд свойств. Некоторые из них очень удобны:

1. Контур-вектор не зависит от параллельного переноса исходного изображения. Поскольку контур кодируется относительно начальной точки, то этот способ кодирования инвариантен сдвигу исходного контура.
2. Поворот изображения на определенный угол равносилен повороту каждого ЭВ контура на тот же угол.
3. Изменение начальной точки ведет к циклическому сдвигу вектору контура. Поскольку элементарным вектором кодируются относительно предыдущей точки, то понятно, что при изменении начальной точки последовательность элементарного вектора будет та же самая, но первым элементарным вектором будет тот, который начинается в начальной точке.
4. Изменение масштаба исходного изображения можно рассматривать как умножение каждого элементарного вектора контура на масштабный коэффициент.

Введем еще несколько понятий. Скалярным произведением контуров Г и N называется такое комплексное число:

![](./resources/doc/contour-doc-2.png?raw=true)

где k – размерность вектор-контура;
Yn — n-й элементарный вектор контура Г;
Vn — n-й элементарный вектор контура N;
(Yn, Vn) — скалярное произведение комплексных чисел.

нормированное скалярное произведение (НСП):

![](./resources/doc/contour-doc-3.png?raw=true)

где |Г| и |N| — нормы(длины) контуров, вычисляемые как:

![](./resources/doc/contour-doc-4.png?raw=true)

НСП в пространстве комплексных чисел, также является комплексным числом. При этом, единица – это максимально возможное значение модуля НСП (это следует из неравенства Коши-Буняковского: |ab|*|a||b|), и она достигается только если:

![](./resources/doc/contour-doc-5.png?raw=true)

где µ – произвольное комплексное число.
При умножении комплексных чисел, их модули (длины) перемножаются, а аргументы (углы) – складываются. Значит контур N это тот же контур N, но повернутый и промасштабированный. Масштаб и поворот определяется комплексным числом. Итак, модуль НСП достигает максимального значение – единицы, только если контур Г является тем же контуром N, но повернутым на некоторый угол и про масштабированный на определенный коэффициент.

НСП является чрезвычайно полезной формулой для поиска похожих между собой контуров. К сожалению, есть одно обстоятельство не позволяющее использовать его напрямую. И это обстоятельство – выбор начальной точки. Равенство достигается только если начальные точки контуров – совпадают. Если же контуры одинаковы, но отсчет ЭВ начинается с другой начальной точки, то модуль НСП таких контуров не будет равен единице. Введем понятие взаимокорреляционной функции (ВКФ) двух контуров:

![](./resources/doc/contour-doc-6.png?raw=true)

Значения этой функции показывают насколько похожи контуры Г и N, если сдвинуть начальную точку N на m позиций. ВКФ определена на всем множестве целых чисел, но поскольку циклический сдвиг на k приводит нас к исходному контуру, то ВКФ является периодической, с периодом k. Поэтому нас будет интересовать значения этой функции только в пределах от 0 до k-1. Найдем величину, имеющую максимальный модуль среди значений ВКФ:

![](./resources/doc/contour-doc-7.png?raw=true)

Из определений НСП и ВКФ, понятно, что Ψmax является мерой похожести двух контуров, инвариантной переносу, масштабированию, вращению и сдвигу начальной точки. При этом, модуль |max| показывает степень похожести контуров, и достигает единицы для одинаковых контуров, а аргумент arg (max) дает угол поворота одного контура, относительно другого.
Введем еще одно понятие – автокорреляционной функции (АКФ). Автокорреляционная функция является ВКФ для которой N=Г. По сути – это скалярное произведение контура самого на себя при различных сдвигах начальной точки:

![](./resources/doc/contour-doc-8.png?raw=true)

Вот некоторые свойства АКФ.
1. АКФ не зависит от выбора начальной точки контура. Действительно, посмотрим на определение скалярного произведения (1). Как видим, изменение начальной точки приведет просто к изменению порядка суммируемых элементов и не приведет изменению суммы. Этот вывод не столь очевиден, но если вдуматься в смысл АКФ, то он ясен.
2. Модуль АКФ симметричен относительно центрального отсчета k/2. Поскольку АКФ является суммой попарных произведений ЭВ контура, то каждая пара встретится два раза на интервале от 0 до k. 

АКФ контура можно считать характеристикой формы контура[[9]](#literature-9). Так, формы, близкие к кругу имеют равномерные значения модуля АКФ. Сильно вытянутые в одном направлении формы – имеют провал в центральной части АКФ (см рисунок прямоугольника). Формы, переходящие в самих себя при повороте, имеют максимум АКФ в соответствующем месте. Нормированная АКФ не зависит от масштаба, положения, вращения и выбора начальной точки контура. Это следует из пункта 1-го и из свойств НСП.

**Но АКФ все равно достаточно велика для использования в качестве сигнатуры(дексриптора)**

В идеале — дескриптором должно быть одно число.
АКФ представляет собой вектор с k/2 значениями. Если вычисляем некую меру различий двух АКФ, нам нужно перебрать все k/2 значений. В принципе, если порог различий фиксирован, то мы можем на каждом шагу вычислять текущую меру различий, и как только она превысит порог – прерывать сравнение АКФ. Таким образом мы ускорим сравнение АКФ. Однако тут есть две сложности – во-первых, вычисление меры на каждом шагу затратно по времени, во-вторых, если различие происходит в середине, или только в последних компонентах значении АКФ, то эффективность данного алгоритма падает, и он вырождается в полное сравнение. Для более эффективного решения проблемы сравнения АКФ, изменим способ представления АКФ. Вместо хранения самой АКФ, будем хранить вейвлетную свертку модулей значений АКФ. При этом первый компонент свертки будет соответствовать наиболее крупномасштабному вейвлету, а последующие компоненты – будут уточнять значения функции во все более мелких масштабах.
Вейвлетная свертка позволит нам упорядочить значения АКФ в масштабном порядке. Первым будет идти компонент, отвечающий наиболее крупномасштабным особенностям АКФ, а дальнейшие компоненты будут уточнять все более мелкие особенности АКФ.
В принципе, можно использовать любые свертки, у которых первый компонент характеризует функцию в наибольшем масштабе, а последующие – уточняют значение на более мелких масштабах.
Однако, учитывая то, что свертка нам нужна для сравнения, и то, что АКФ часто является симметричной функцией (см рисунки в первой главе), то не все вейвлеты одинаково хорошо подходят для наших целей. Основным критерием к выбору вейвлета является его дискриминирующие свойства. Для примера, рассмотрим широко распространенные вейвлеты Хаара и Уолша:

![](./resources/doc/ris3.jpg?raw=true)

В данном случае использовалась свертка Уолша. Она не дает равномерного распределения, однако ее дискриминирующих свойств первого компонента достаточно для уменьшения пространства поиска шаблонов в три-четыре раза.

**В реальном же изображении контуры имеют произвольную длину. Поэтому, для поиска и сравнения контуров, все они должны быть приведены к единой длине.**

Будем применять более быстрый способ эквализации, который, помимо приведения к единой длине, производит сглаживание контура методом скользящего среднего. Итак, сначала мы фиксируем длину ВК, которую мы будем использовать в нашей системе распознавания. Обозначим ее k. Затем, для каждого исходного контура Г создаем вектор-контур N длиной k. Далее возможно два варианта – либо исходный контур имеет большее число ЭВ чем k, либо меньшее число чем k. Если исходный контур больше необходимого, то перебираем все его ЭВ, и считаем элементы N как сумму всех ЭВ. Этот алгоритм достаточно грубый, особенно для длин немногим больших k, однако он вполне применим на практике.

При малых значениях k (менее 30) — резко повышается число шумовых распознаваний (распознавание как символов шума или других не символьных элементов изображения), снижается число верных распознаваний, и увеличивается число ложных распознаваний. Таким образом, значение k=30 является оптимальным для данной системы распознаваний.

В результате всех манипуляций мы получим вменяемую сигнатуру для каждой цифры. Вектор состоящий из 4-х элементов.

## <a name="algo-precendent"></a> Обучение по методу ближайших соседей и адаптивные веса

Дальшейший шаг — обучение и распознавание цифр.

Сильно углублятся в данном разделе не вижу особого смысла. Использовался простейший метрический классификатор основанный на оценивании сходства объектов (метод ближайших соседей). Классифицируемый объект относится к тому классу, которому принадлежат ближайшие к нему объекты обучающей выборки.[[8]](#literature-8)
В качестве метрики близости было выбрано эвклидово расстояние в 4-х мернном пространстве (см. сигнатуру из контурного анализа). 

*Адаптивные веса для классов*

Как показала практика, некоторые цифры более похижи между собой, а некоторые меньше. Тут проявляется первый недостаток выбранного алгоритма обучения по методу ближайших соседей. В отличии от нейронных сетей ему не хватает адаптивности. 

Данная проблема решается с помощью усовершенствования метода ближайших соседей — Метода потенциальных функций. Он позволяет с помощью простого алгоритма оценивать вес («важность») объектов обучающей выборки при решении задачи классификации. Общая идея метода иллюстрируется на примере электростатического взаимодействия элементарных частиц. Известно, что потенциал («мера воздействия») электрического поля элементарной заряженной частицы в некоторой точке пространства пропорционален отношению заряда частицы (Q) к расстоянию до частицы (r). Метод потенциальных функций реализует полную аналогию указанного выше примера. При классификации объект проверяется на близость к объектам из обучающей выборки. Считается, что объекты из обучающей выборки «заряжены» своим классом, а мера «важности» каждого из них при классификации зависит от его «заряда» и расстояния до классифицируемого объекта.

При сравнении использовался подход «Один против всех». Значит на каждом шаге сравнивались со всеми уже обученными результатами.

Непосредственно для обучения использовался метод *скользящегоконтроля*. Фиксируется некоторое множество разбиений исходной выборки на две подвыборки: обучающую и контрольную. Для каждого разбиения выполняется настройка алгоритма по обучающей подвыборке, затем оценивается его средняя ошибка на объектах контрольной подвыборки. Оценкой скользящего контроля называется средняя по всем разбиениям величина ошибки на контрольных подвыборках.

Итоговые обученные веса

                    Цифра |   0   |   1   |   2   |   3   |   4   |   5   |   6   |   7   |   8   |   9
------------------------: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: 
MaxACFDescriptorDeviation |   5   |  10   |  10   |  10   |   10  |   8   |   8   |  10   |   5   |   5

Тут виден серьезный недостаток метода. Если бы классов было значительно больше или вообще неопределнное количество(кластеризация) -- ни о какой адаптивности речи идти и не могло бы.

## <a name="algo-hacks"></a> Корректировки и хаки

Как мы знаем из раздела Контураный анализ, контуры инвариантны к повороту. Потому есть некоторые побочные эффекты, которые следует компенсировать.

1. «6» это по сути повернутая на 180 градусов «9». Потому ограничим максимальный угол поворота на значение π/2
2. В текущем шрифте цифра «1» это по сути немного повернутая цифра «7». Потому ограничим угол значением π/6.5
3. Из-за того что изображение сильно зашумлено аддитивным гаусовским шумом, контуры «6», «8», «9», «0» могут быть очень близки. Для распознавания цифр «6», «8», «9», «0» на цифрах дополнительно к общему алгоритму распознавания выделены специальные областикоторые однозначно характеризуют ее. По сути для некоторых пар использовался подход «Все против всех». Используется только в очень спорных случаях.

![](./resources/doc/hack6.png?raw=true) ![](./resources/doc/hack8.png?raw=true) ![](./resources/doc/hack9.png?raw=true)

Как видим, из-за зашумлленности и недостаточной адаптивности приходится применять различные ухищрения. Это печалит.

# <a name="test"></a> Результаты экспериментов

## <a name="test-suppose"></a> Данные и гипотезы

Чтоб показать сходимость обучения был выбран подход:
>_Научить на 100 изображениях и проверять нераспознанные и ошибочно распознанные изображения на следующей сотне._

Под термином «обучение» подразумевается добавление в базу нового прецендента *одной цифры*. Для проверки обучения было принято придерживаться правила 80/20, 80% в качестве обучающей выборки и 20% в качестве проверочной.

## <a name="test-training-hard"></a> Обучение на «жестких весах»

Веса указаны в конфигурации ```DigitSymbolRecogniser.exe.hard.config```

                    Цифра |   0   |   1   |   2   |   3   |   4   |   5   |   6   |   7   |   8   |   9
------------------------: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: 
MaxACFDescriptorDeviation |   2   |   2   |   2   |   2   |    2  |   2   |   2   |   2   |   2   |   2


              Выборка | 0-100 | 101-200 | 201-300 | 301-400 | 401-500 | 501-600 | 601-700 | 701-800 | 801-900 | 901-1000 | 1001-1100 | 1101-1200
--------------------: | :---: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :------: | :-------: | :-------:
             Обучение |  225  |   129   |   90    |   78    |   72    |   74    |   69    |   57    |   57    |    52    |    49     |    52
       Нераспознанное |       |   83    |   61    |   57    |   49    |   54    |   41    |   43    |   39    |    39    |    36     |    47
Ошибочно распознанное |       |    4    |    6    |    7    |    9    |    5    |   10    |    8    |    5    |    6     |    12     |     6

*График*
![](./resources/doc/test-training-hard.png?raw=true)

>**Как видим, наблюдается:**

> - Схождение обучения к условным 50 значениям на 100 изображениях(50 прецендентам на 500 цифр)
> - Практически полное накладывание графика нераспознаного на график обучения, что весьма логично, учитывая то что обучаем мы преценденты в основном на нераспознанных изображениях
> - Небольшое увеличение ошибочно распознаных изображений. Действительно, чем больше обучаем - тем больше должно появляться ошибок

## <a name="test-verification-hard"></a> Проверка на «жестких весах»

В качестве обучающей выборки тут было взято уже существующую выборку из 1200 значений и следующие 300 значений в качестве тестовой.
Результат был таков:
* Нераспознано - 100 изображений (Полнота — 67%)
* Ошибочно распознано - 18 изображений (Точность 94%)

Результат вышел неудовлетворительным. бучение сходится к «50 прецендентам на 500 цифр», что слишком много. 

К тому же полнота всего 67%, а это значит что при распознавании ищется слишком близкое соответствие обученному преценденту. В следствие этих причин было принято решение смягчить веса и повторить эксперимент.

## <a name="test-training-soft"></a> Обучение на «мягких весах»

Веса указаны в конфигурации ```DigitSymbolRecogniser.exe.soft.config```

                    Цифра |   0   |   1   |   2   |   3   |   4   |   5   |   6   |   7   |   8   |   9
------------------------: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: 
MaxACFDescriptorDeviation |   5   |  10   |  10   |  10   |   10  |   8   |   8   |  10   |   5   |   5


              Выборка | 0-100 | 101-200 | 201-300 | 301-400 | 401-500 | 501-600 | 601-700 | 701-800 | 801-900 | 901-1000 | 1001-1100 | 1101-1200
--------------------: | :---: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :------: | :-------: | :-------:
             Обучение |  136  |   65    |   43    |   26    |   35    |   27    |   30    |   26    |   25    |    24    |    18     |    20
       Нераспознанное |       |   57    |   42    |   26    |   34    |   25    |   34    |   26    |   23    |    23    |    16     |    20
Ошибочно распознанное |       |    3    |    8    |    7    |   10    |   12    |   12    |    7    |    8    |    9     |     7     |    13

*График*
![](./resources/doc/test-training-soft.png?raw=true)

>**Наблюдения:**

> - Схождение обучения к условным 20 значениям на 100 изображениях(20 прецендентам на 500 цифр)
> - То же накладывание графика нераспознаного на график обучения
> - Близкие значения ошибочно распознаных изображений. Это говорит о том что «мягкие» коэффициенты были взяты корректно. Не допущени излишняя мягкость.

## <a name="test-verification-soft"></a> Проверка на «мягких весах» 

В качестве обучающей выборки тут было взято уже существующую выборку из 1200 значений и следующие 300 значений в качестве тестовой.
Результат был таков:
* Нераспознано - 59 изображений (Полнота — 80%)
* Ошибочно распознано - 16 изображений (Точность 95%)

Результат вышел получше чем в прошлый раз но все равно неудовлетворительным. Обучение сходится к «20 прецендентам на 500 цифр», это допустимо. 

Полнота достигает 80%, а это все еще слишком мало и не то чего я хотел достичь.  

Потому было принято решение применить «жестко-мягкий» подход. Я обучу на «жестких» весах, но для проверки и распознавании буду использовать «мягкие» веса. Надеюсь это позволит получить большую обученную базу и ее разнообразие и, тем самым, достичь лучшего процента распознанных изображений.

## <a name="test-soft-hard"></a> Обучение на «жестких» и проверка на «мягких» весах

*Обучение*

              Выборка | 0-100 | 101-200 | 201-300 | 301-400 | 401-500 | 501-600 | 601-700 | 701-800 | 801-900 | 901-1000 | 1001-1100 | 1101-1200 | 1201-1300 | 1301-1400 | 1401-1500
--------------------: | :---: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :------: | :-------: | :-------: | :-------: | :-------: | :-------:
             Обучение |  225  |   129   |   90    |   78    |   72    |   74    |   69    |   57    |   57    |    52    |    49     |    52     |     41    |     42    |    35
       Нераспознанное |       |   83    |   61    |   57    |   49    |   54    |   41    |   43    |   39    |    39    |    36     |    47     |     35    |     34    |    28
Ошибочно распознанное |       |    4    |    6    |    7    |    9    |    5    |   10    |    8    |    5    |    6     |    12     |     6     |     6     |     8     |     6


*График*
![](./resources/doc/test-training-hard-soft.png?raw=true)

*Итоговое количество записанных прецендентов*

 Класс | Количество
-----: | :---------
   0   |    32
   1   |    188
   2   |    93
   3   |    142
   4   |    142
   5   |    142
   6   |    108
   7   |    135
   8   |    44
   9   |    129
  6890 |    58

Ничего нового по сравнени с обучением на «жестких» весах не видно. Количество ошибочно распознаных изображений сильно не растет, а держится приблизительно на одном уровне.
В этот было выбрано 1500 значений в качестве обучающей выборки и несколько тестовых выборок по 375 значений.

Вот результат по тестовым выборкам

                     Выборка | 1501-1875 | 1876-2250 | 2251-2625 
---------------------------: | :-------: | :-------: | :--------:
              Нераспознанное |     14    |     1     |    6    
       Ошибочно распознанное |     22    |     22    |    20   
Доля НЕправильных ответов(%) |    9.6    |     6     |    7    
			 
_Результат вышел достачно неплохим_. Процент нераспознаного или ошибочно распознаного держится не выше 10%.
Особенно радует тот факт что нераспознанного достаточно мало. Больше ошибочно распознанного. 
Дело в том что в выборке есть некоторое количество картинок где человек тоже не может гарантировано распознать изображение (настолько оно зашумлено).

Можно сделать вывод что «жестко-мягкий» подход хоро себя показывает на задачах классификации.

## <a name="test-full"></a> Общий результат на выборке в 3000

Для уверенности была протестирована еще выборка в 2946 изображений.

>**Общий резуольтат: Нераспознано или ошибочно распознано 294 изображений**
Это означает 93% точности и 96% полноты при распознавании капчи и 99% !!! точности и 99% полноты для отдельной цифры. 
Считаю этот результат достаточно удовлетворительным.

# <a name="conclusion"></a> Выводы

Контурный анализ, как продвинутый алгоритм ближайшего соседа (можно сравнить с методом парзеновского окна, потенциальных функций [[6]](#literature-6), алгоритм STOLP [[7]](#literature-7)) можно использовать на задачах классификации с известным и небольшим количеством классов. Желательно равновероятных, хотя есть и уловки которыми я воспользовался для избежания данной проблемы. Вопреки расхожему мнению о низкой точности метода была достигнута точность 90%. Потому, считаю что контурный анализ следует анализировать не как метод ближайшего соседа, а как самостоятельный метод классификации. Кроме того подход контурного анализа позволяет эффективно выделять сигнатуры контуров на изображении и избежать огромной размерности пространства признаков.

К преимуществам метода можно отнести:

* Отсутствие проблемы переобучения. Сам по себе контурный анализ очень эффективно справляется с выделением сигнауры изображений
* Скорость работы.
* Инвариантность к повороту обьекта.
* Простота реализации.
* Простота обучения.
* Простота модификации. Смотри [Корректировки и хаки](#algo-hacks)

Недостатками метода являются 

* Изначально классы считаются равновероятными. Но эта проблема может быть решена с помощью внедрения «весов» для каждого класса, что было сделано в данной работе.
* Сильная потребность в предварительной обработке. Разомкнутый или наоборот лишний раз сомкнутый контур очень сильно портят процесс распознавания.
* Медленная сходимость. Потребовалось около 1200 jобучений всего для 10 классов. Антилидер класс «1» с 188 обучениями.
* Сложная настройка весов (Практически ручная)
* Ручное внедрение весов для каждого класса. Делает невозможным применение метода для задач кластеризации с заранее неизвестным количеством классов.

В результате можно сказать что применять эту методику стоит если вам нужно какая-то не сильно сложная система распознавания с небольшим количеством классов(текущая задача). Для более серьезных вещей применить будет затруднительно.

# <a name="literature"></a> Литература

<a name="literature-1"></a> 1. [Рафаэл С. Гонсалес, Ричард Е. Вудс Цифровая обработка изображений, Техносфера Москва 2005 Pp 335](./resources/literature/Cifrovaya_obrabotka_izobrageniy.djvu)

<a name="literature-2"></a> 2. [Giuseppe Papari, Nicolai Petkov, and Patrizio Campisi, Artistic Edge and Corner Enhancing Smoothing, IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 10, OCTOBER 2007, pages 2449-2461](./resources/literature/TIP_artistic.pdf)

<a name="literature-3"></a> 3. [Shapiro, L. G. & Stockman, G. C: "Computer Vision", page 137, 150. Prentice Hall, 2001](./resources/literature/Computer-Vision-Linda-Shapiro.pdf)

<a name="literature-4"></a> 4. [Canny, J., A Computational Approach To Edge Detection, IEEE Trans. Pattern Analysis and Machine Intelligence, 8(6):679–698, 1986.](./resources/literature/canny1986.pdf)

<a name="literature-5"></a> 5. [Фурман Я.А. (ред.) Введение в контурный анализ. Приложения к обработке изображений и сигналов, М.: ФИЗМАТЛИТ, 2003](./resources/literature/Vvedenije_v_konturnyj_analiz_2003.djvu)

<a name="literature-6"></a> 6. [Айзерман М. А., Браверман Э. М., Розоноэр Л. И. Метод потенциальных функций в теории обучения машин. — М.: Наука, 1970. — 320 pp.](./resources/literature/The-method-of-potential-functions-in-machine-learning-theory.djvu)

<a name="literature-7"></a> 7. [Вапник В. Н., Червоненкис А. Я. Теория распознавания образов. — М.: Наука, 1974.](./resources/literature/Vapnik.djvu)

<a name="literature-8"></a> 8. [Воронцов К.В. Математические методы обучения по прецедентам (теория обучения машин) Pp. 43-55](./resources/literature/Voron-ML-1.pdf?raw=true)

<a name="literature-9"></a> 9. [Потапов А.А., Гуляев Ю.В., Никитов С.А., Пахомов А.А., Герман В.А. Новейшие методы обработки изображений - М.: ФИЗМАТЛИТ, 2008. –489 ](./resources/literature/50999.pdf?raw=true)

# <a name="license"></a> Лицензия

Распространяется под [MIT license](LICENSE)
Проще говоря, как есть:)
